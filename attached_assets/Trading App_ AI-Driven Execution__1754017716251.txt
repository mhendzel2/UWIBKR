Architectural Blueprint for an AI-Powered Options Trading System




Part I: System Architecture and Foundational Principles


The development of a sophisticated trading application requires a transition from the mindset of scripting to that of system architecture. A simple, linear script that fetches data, analyzes it, and executes a trade is inherently fragile and incapable of handling the asynchronous and unpredictable nature of live financial markets. This report outlines a professional-grade architectural blueprint designed for robustness, scalability, and, above all, rigorous risk management. The architecture is founded on principles of modularity and event-driven design, ensuring that each component can operate independently and resiliently, forming a cohesive and powerful whole.


Section 1.1: A Professional-Grade Algorithmic Trading Architecture


The foundation of any robust algorithmic trading system is its architecture. A monolithic design, where all logic is intertwined, is prone to failure; an error in one part, such as a temporary loss of connection to the brokerage, can bring the entire system down. To avoid this, the proposed system will be designed using an Event-Driven Architecture (EDA). An EDA is a software design pattern that promotes the production, detection, consumption of, and reaction to events.1 In a trading context, an "event" can be a new piece of market data, a trade execution confirmation, a change in account status, or a signal generated by an analytical model. This approach naturally decouples the system's components, allowing for greater resilience, scalability, and maintainability.
The system will be conceptually divided into distinct layers, each with a single responsibility. This modularity is inspired by the principles of Service-Oriented Architecture (SOA), where components are independent and communicate through well-defined interfaces.2 For a single-machine deployment, this communication can be handled by an in-memory message queue (such as Python's
asyncio.Queue), while a multi-machine deployment could leverage a more robust message broker like RabbitMQ.
The System's Conceptual Layers:
* Data Ingestion Layer: This layer's sole responsibility is to interface with the Unusual Whales API. It continuously polls for new options flow, gamma exposure data, and custom alerts. Upon receiving raw data, it does not process it but instead publishes it as a "raw data event" onto an internal message queue. This isolates the complexities of external API communication and rate-limit handling.
* Processing & Signal Generation Layer: This component acts as a consumer of "raw data events." It leverages the high-performance Polars library to clean, transform, and enrich the data. Its primary function is to apply a set of predefined rules to identify potentially significant market activity—our initial "whale" signals. It then engineers new features from this data, packages it into a structured format, and publishes a "processed data event" for consumption by the intelligence layer.
* Intelligence Core: This is the analytical brain of the system, centered around the Google Gemini Large Language Model (LLM). It consumes "processed data events" and is tasked with performing a deeper, more nuanced analysis. It synthesizes the options flow, gamma positioning, and market context to generate potential, defined-risk trade ideas. The output of this layer is not a direct order but a "trade hypothesis event."
* Risk & Validation Layer: This is arguably the most critical layer in the entire system. It sits as a mandatory gatekeeper between the Intelligence Core and the Execution Gateway. It consumes "trade hypothesis events" and subjects them to a battery of automated checks. These include validating the logical and numerical soundness of the AI's suggestion, cross-referencing it with live market data, and ensuring it complies with the user's predefined risk parameters. Only a fully validated and vetted hypothesis is passed along as a "confirmed trade idea event."
* Execution Gateway: This layer is a dedicated service that encapsulates all interactions with the Interactive Brokers (IBKR) Trader Workstation (TWS) API. It manages the persistent connection, handles order lifecycle management (placement, modification, cancellation), and monitors account and position updates. It consumes "confirmed trade idea events" only after they have been explicitly approved by the human user.
* User Interface (UI) / Control Panel: While the focus of this report is the backend architecture, the UI is the crucial point of human oversight. It presents "confirmed trade ideas" for final approval, displays current positions, account P&L, and system status alerts. The user's confirmation is the final event that triggers the Execution Gateway to place a live order.
This decoupled, event-driven structure provides immense advantages over a simple script. For instance, if the call to the Gemini API in the Intelligence Core is slow or fails, it does not halt the Data Ingestion Layer from continuing to receive and queue up new market data. The system remains responsive and does not lose its place in the market. This resilience is a hallmark of professional trading system design.1


Section 1.2: Mastering Latency and Performance


In algorithmic trading, latency—the delay between an event and the system's reaction to it—is a critical performance metric. While this system is not designed for the nanosecond-sensitive world of high-frequency trading (HFT), minimizing delay is still crucial for acting on timely information. The primary sources of latency in this architecture are 3:
1. Network Latency: The time required for data to travel between the trading server and the external API servers (Unusual Whales, Gemini, IBKR). This is largely a function of physics and geography. The most effective mitigation is strategic server hosting—deploying the application on a Virtual Private Server (VPS) located in a data center physically close to the API endpoints, such as those in the New York/New Jersey area for US financial markets.3
2. Processing Latency: The time taken for internal computations. With Polars, this is minimized. Polars is built in Rust and designed for multi-threaded, parallel execution, making it significantly faster than single-threaded alternatives for data manipulation.5 The most significant source of processing latency will be the network-bound, computationally intensive call to the Google Gemini API.
3. Execution Latency: The time from when an order is sent to IBKR to when it is acknowledged and executed by the exchange. This is managed by the broker's infrastructure.
The architectural solution to managing these latencies, particularly the bottleneck of the Gemini API call, is the pervasive use of asynchronous operations. The entire application must be built using a framework like Python's asyncio. This allows the system to perform non-blocking I/O operations.
Consider the signal-to-trade pipeline. A synchronous, or blocking, implementation would execute as follows:
1. await get_unusual_whales_data()
2. process_data_with_polars()
3. await analyze_with_gemini()
4. await user_confirm_trade()
5. await execute_with_ibkr()
The call to analyze_with_gemini() could take several seconds. During this time, a synchronous program would be completely frozen, unable to process any new, incoming market data. By the time the Gemini analysis is complete, the market state that prompted the analysis may have changed entirely, rendering the trade idea obsolete.
An asynchronous, non-blocking architecture solves this problem. When the Processing Layer sends a request to Gemini, it doesn't wait for the response. Instead, it yields control back to the asyncio event loop, which can then immediately process the next piece of incoming data from the queue. The system can have multiple requests to Gemini "in-flight" simultaneously. When a response from Gemini eventually arrives, the event loop directs it to the Risk & Validation Layer. This ensures that the system remains responsive and that the data pipeline does not get clogged by its slowest component. This design is not merely an optimization; it is a fundamental requirement for the application to function effectively in a live trading environment.


Section 1.3: The Bedrock: A Multi-Layered Risk Management Framework


Risk management is the single most important aspect of any trading system, automated or otherwise. A system that can generate phenomenal profits but has no safeguards against catastrophic loss is a failed system. This architecture embeds risk management at every level, creating a multi-layered defense against technical glitches, market anomalies, and model errors.6
Pre-Trade Risk Controls:
These are automated checks implemented in the code that run on every single order before it is transmitted to the broker. They are the first line of defense against erroneous trades.
* Parameter Sanity Checks: Before constructing an order, the system will validate all parameters. Is the quantity positive? Is the limit price for a debit spread a realistic debit, not a credit? Is the expiration date in the future? These simple checks prevent a wide range of bugs from resulting in malformed orders.
* Position Sizing: The system will enforce a strict position sizing rule, such as the fixed fractional method.7 Before placing any trade, the application will calculate the maximum potential loss (for defined-risk spreads, this is the net debit paid). This loss will be compared against a user-defined percentage (e.g., 1%) of the total account equity. If the potential loss exceeds this threshold, the trade is blocked. This prevents any single trade from having an outsized negative impact on the portfolio.
   * Example Logic: max_loss = net_debit * quantity * 100. if max_loss > (account_equity * 0.01): raise RiskViolationError("Trade exceeds max risk per trade.")
* "Fat-Finger" and Concurrency Prevention: The system will have a global maximum for contract quantity per trade (e.g., MAX_CONTRACTS_PER_TRADE = 50). It will also implement a locking mechanism to prevent the accidental submission of the same trade multiple times due to rapid user clicks or a software glitch.
In-Flight Risk Controls:
These are real-time monitoring processes that run continuously while the application is active.
* Maximum Drawdown Limit: The application will subscribe to real-time account P&L updates from IBKR. It will maintain a running calculation of the day's net profit or loss. If this value drops below a pre-configured maximum drawdown threshold (e.g., a 5% loss on the starting day's equity), the system enters a "risk-off" state. In this state, it will automatically cancel all open working orders and prevent any new orders from being placed until manually reset by the user.7 This acts as a master circuit breaker for the entire strategy.
* Connectivity Monitoring: The health of the API connections to both Unusual Whales and IBKR is critical. The system will have dedicated tasks that periodically send "heartbeat" requests to these services. If a connection is lost, the system will immediately enter the "risk-off" state, preventing it from trading blind based on stale data. It will attempt to reconnect gracefully and will only resume trading after all connections are confirmed to be stable.
Model Risk Controls:
This is a unique risk vector introduced by the use of an LLM. An LLM can "hallucinate" or provide factually incorrect information with high confidence.9 Blindly executing its suggestions is unacceptably risky. A dedicated validation layer for the AI's output is therefore essential. This will be detailed further in Section 3.3, but its place in the overall risk framework is established here. It includes checks for numerical accuracy, logical consistency, and cross-referencing with real market data before a trade idea is ever presented to the user for confirmation.11


Part II: The Data Engine: Ingestion and High-Performance Processing


The performance and quality of a trading system are inextricably linked to the quality and timeliness of its data. This section details the architecture of the data engine, covering the acquisition of raw data from the Unusual Whales API and its subsequent high-performance processing and feature engineering using the Polars library. The focus is on efficiency, correctness, and extracting the most valuable signals from the noise.


Section 2.1: Taming the Firehose: Advanced Unusual Whales API Integration


The Unusual Whales API provides a comprehensive and rich source of options market data.12 Effectively harnessing this data requires a strategic approach to endpoint selection and a robust data acquisition loop.
Authentication and Security:
All API requests must include a valid Bearer token in the Authorization header of the HTTP request.13 It is critical that this API key is not hard-coded into the source code. It should be managed as a secure secret, for example, by loading it from an environment variable or a dedicated secrets management service.
Strategic Endpoint Selection:
A naive approach might be to consume the entire options tape, but this is inefficient and noisy. The key to building an effective system is to select the endpoints that provide the highest quality, pre-filtered information relevant to the strategy. The following table outlines the curated set of endpoints that will form the core of the Data Ingestion Layer.


Endpoint
	Primary Use Case
	Update Frequency
	Key Data Points
	Notes/Caveats
	/option-trades/flow-alerts
	Identifying significant, "urgent" options flow (whales)
	Real-time (push/poll)
	total_premium, total_size, open_interest, alert_rule (e.g., RepeatedHits), ask_side_percentage
	Pre-filtered by the Unusual Whales algorithm, this endpoint significantly reduces noise and focuses on institutionally relevant activity. This should be the primary trigger for new trade analysis. 14
	/stock/:ticker/spot-exposures/strike
	Real-time GEX, DEX, Vanna, Charm by strike
	Real-time (poll)
	strike, call_gamma_exposure, put_gamma_exposure, net_gamma_exposure
	Crucial for intraday volatility analysis and identifying price "magnets." This endpoint provides directionalized volume-based gamma, which reflects intraday positioning changes and is superior to static, OI-based calculations for tactical trading. 12
	/stock/:ticker/greeks
	Daily snapshot of Greeks based on Open Interest
	Daily
	gex, dex, etc. for the whole ticker
	Provides a baseline macro view of market maker positioning at the start of the trading day. Useful for establishing the overall gamma regime (positive or negative). 12
	/alerts
	Retrieving user-defined custom alerts
	Real-time (poll)
	name, noti_type, meta (alert details)
	Allows the application to react to alerts the user sets up on the Unusual Whales website, creating a tight, personalized feedback loop between the user's manual research and the automated system. 17
	/stock/:ticker/stock-state
	Last stock price and volume
	Real-time
	price, volume
	Essential context needed for calculations like moneyness and for providing the AI with the most current state of the underlying asset. 13
	Data Acquisition Loop:
The Data Ingestion Layer will run a persistent asyncio loop. This loop will be responsible for polling the selected endpoints at appropriate intervals. The design must incorporate:
* Robust Error Handling: The loop must gracefully handle potential network issues, such as HTTP errors (e.g., 503 Service Unavailable), timeouts, or malformed JSON responses. It should implement a retry mechanism with exponential backoff.
* Rate Limit Management: The Unusual Whales API enforces rate limits on requests per minute and per day. The application must respect these limits to avoid being blocked. The acquisition loop will inspect the response headers of every successful API call, specifically x-uw-req-per-minute-remaining and x-uw-daily-req-count, to dynamically manage its polling frequency and stay within the allowed limits.18
By using the pre-filtered /option-trades/flow-alerts endpoint as the primary trigger, the system intelligently leverages the work already done by the Unusual Whales platform. It focuses its analytical resources on trades that are already algorithmically determined to be significant, rather than wasting cycles analyzing every single trade on the tape. The addition of real-time, directionalized gamma exposure provides a powerful, dynamic layer of context that is often missing from retail trading tools.


Section 2.2: High-Performance Data Manipulation with Polars


Once raw data is ingested, it must be rapidly processed, cleaned, and transformed into a format suitable for analysis. For this task, the Polars library is the optimal choice. Its performance, stemming from a multi-threaded Rust core and an intelligent query optimizer, far exceeds that of traditional single-threaded libraries like pandas, which is a critical advantage in time-sensitive financial analysis.5
Schema Enforcement and Data Type Conversion:
The JSON data returned by the API is untyped. The first and most critical step in Polars is to ingest this data into a DataFrame with a strict, well-defined schema. This involves explicitly casting columns from their default string or generic number representation to the correct, precise data types using the .cast() method.21
* Numerical fields like total_premium or underlying_price should be cast to pl.Float64.
* Integer fields like total_size or open_interest should be cast to pl.Int64.
* Date/time fields, often represented as string timestamps, must be parsed and cast to pl.Datetime. Polars provides powerful tools for this conversion.
A professional optimization technique is to use the smallest necessary data type for each column to conserve memory and increase processing speed.23 For example, if
days_to_expiration will never exceed a few thousand, it can be stored as pl.UInt16 instead of the default pl.Int64, reducing memory usage for that column by 75%.
From Filtering to Advanced Feature Engineering:
The true power of Polars lies in its expressive API, which allows for complex transformations and feature engineering to be chained together in a single, optimized query.24 Instead of just filtering rows, the goal is to create new, insightful features that provide richer context for the AI model. This pre-digestion of information makes the AI's task easier and its analysis more potent.
The following conceptual Polars pipeline demonstrates how to move from raw flow data to a high-conviction, feature-rich dataset ready for Gemini:


Python




import polars as pl
from datetime import datetime

# Assume 'raw_flow_df' is a Polars DataFrame created from the 
# /option-trades/flow-alerts API response, with initial type casting done.

# A single, chained expression for filtering and feature engineering
high_conviction_flow = (
   raw_flow_df
   # 1. Filtering for high-conviction signals
  .filter(
       (pl.col("total_premium") >= 250_000) &  # Rule 1: Significant monetary value
       (pl.col("total_size") > pl.col("open_interest")) &  # Rule 2: Likely an opening trade [26, 27]
       (pl.col("ask_side_percentage") > 0.80) & # Rule 3: Strong buying aggression [28, 29]
       (pl.col("alert_rule").str.contains("Repeat|Ascending")) & # Rule 4: Urgency signal from UW [15]
       (pl.col("dte").is_between(30, 90)) # Rule 5: Focus on strategic (non-weekly) timeframes
   )
   # 2. Engineering new features for richer context
  .with_columns(
       # Feature: Average price paid per contract, a measure of conviction
       (pl.col("total_premium") / (pl.col("total_size") * 100)).alias("avg_price_per_contract"),
       
       # Feature: Moneyness, indicates how far OTM/ITM the bet is
       ((pl.col("strike") / pl.col("underlying_price")) - 1).alias("moneyness_perc"),
       
       # Feature: Side of the trade based on premium distribution
       pl.when(pl.col("total_ask_side_prem") > pl.col("total_bid_side_prem"))
        .then(pl.lit("BUY"))
        .otherwise(pl.lit("SELL"))
        .alias("inferred_side")
   )
   # 3. Sorting to prioritize the most significant trades
  .sort("total_premium", descending=True)
)

# This resulting 'high_conviction_flow' DataFrame is now ready to be passed to Gemini.

This pipeline accomplishes several things that elevate it beyond a simple filter. It codifies a specific definition of a "high-conviction whale trade" based on multiple confluent factors. More importantly, it uses .with_columns() to engineer new features.30 The
avg_price_per_contract provides a more granular view of the price paid than the total premium alone. The moneyness_perc immediately tells the AI how speculative the trade is—a far out-of-the-money (OTM) call is a very different signal from a deep in-the-money (ITM) one.26 Providing these engineered features directly to the AI reduces its need to infer them, focusing its analytical power on strategy and synthesis rather than basic calculation. This is a crucial step in building a truly intelligent system.


Part III: The Intelligence Core: Signal Generation and Validation


This part of the architecture represents the system's cognitive center. Here, the meticulously processed data from the engine is transformed into actionable intelligence. This involves two distinct but interconnected processes: first, the translation of financial concepts like "whale activity" and "gamma exposure" into the logic of the system; and second, the careful orchestration of the Google Gemini LLM to generate and validate trade hypotheses.


Section 3.1: From Raw Data to Actionable Insight: Decoding Flow and Gamma


Before the AI can be prompted, the system must have a clear, code-defined understanding of the signals it is looking for. This involves translating nuanced market phenomena into a set of logical rules and interpretations.
Defining a "Whale Trade" Signal:
Based on the Polars pipeline developed in Section 2.2, a primary bullish signal is triggered when a call option trade meets a confluence of criteria, indicating it is likely an aggressive, institutional opening purchase:
1. Large Premium: The total_premium is substantial (e.g., > $250,000), signifying a meaningful capital commitment that is unlikely to be retail in origin.32
2. Opening Position: The total_size of the transaction is significantly greater than the pre-existing open_interest for that contract. This is a strong indicator that new positions are being established, not existing ones being closed.26
3. Aggressive Execution: The trade occurs predominantly at or near the ask price. The Unusual Whales API provides fields like ask_side_percentage or total_ask_side_prem which quantify this aggression. A high value suggests the buyer was willing to pay a premium for immediate execution, signaling urgency.28
4. Urgent Routing: The trade is identified as a "sweep" or flagged with a relevant alert_rule like "RepeatedHits" or "RepeatedHitsAscendingFill." Sweeps are orders that are split across multiple exchanges to be filled as quickly as possible, which is another strong indicator of urgency and institutional activity.15
A bearish signal can be inferred from similar activity in put options (large, opening, aggressive buying of puts) or, more subtly, from the aggressive selling of calls (trades executing at the bid).
Interpreting Gamma Exposure (GEX) Data:
The GEX data from the /stock/:ticker/spot-exposures/strike endpoint provides critical context on market structure and potential volatility. The AI's analysis must be grounded in an understanding of these dynamics.36
* Positive GEX Regime: When the net gamma exposure for a stock is positive, it implies that market makers are, in aggregate, long gamma. To maintain a delta-neutral position, they will hedge by selling into strength (as the stock rises) and buying into weakness (as the stock falls). This hedging activity acts as a stabilizing force, tending to suppress volatility and promote range-bound or mean-reverting price action.38 In this environment, strategies like selling premium or using debit spreads with modest price targets may be more appropriate.
* Negative GEX Regime: When net gamma is negative, market makers are short gamma. Their hedging activity becomes destabilizing: they must buy into strength and sell into weakness to remain delta-neutral. This creates a feedback loop that amplifies price moves and increases volatility.38 This environment is more conducive to trend-following or long-volatility strategies. A key level to monitor is the "Gamma Flip" or "Zero Gamma" level, where the regime shifts from positive to negative. A price move below this level can signal an acceleration of downside volatility.39
* Price Pinning: Strikes with very large amounts of gamma exposure (both positive call gamma and negative put gamma) act as "magnets" for the stock price, especially as an expiration date approaches. Market maker hedging activity tends to keep the price tethered to these levels.40 The AI will be specifically instructed to identify these high-GEX strikes as potential price targets for profit-taking or as significant areas of support and resistance.


Section 3.2: Engineering Prompts for Gemini: From "Vibe Coding" to Precision Analysis


Communicating with an LLM for a systematic task requires precision. A vague prompt will yield a vague and unreliable response. The key to leveraging Gemini effectively is "prompt engineering"—crafting a detailed, structured prompt that guides the model to produce a consistent, accurate, and, most importantly, programmatically parsable output.41
The prompt will be constructed in multiple parts to provide comprehensive context and clear instructions:
1. Persona: The prompt will begin by assigning a role to the LLM. This focuses its response style and knowledge base.
   * Example: "You are an expert quantitative options analyst and portfolio manager. Your expertise lies in interpreting institutional options flow, market maker gamma exposure, and identifying defined-risk trading opportunities for sophisticated traders." 42
2. Context: This section explains the overall goal and constraints of the task.
   * Example: "You are analyzing real-time options flow and gamma exposure data to generate high-probability trade ideas. The primary objective is capital appreciation through moderately bullish or volatility-based plays, while strictly adhering to a defined-risk framework. The preferred strategies are Call Debit Spreads for directional bullish theses and Long Calendar Spreads for plays on volatility and time decay."
3. Data Input: The prompt will include the prepared data, clearly labeled. The Polars DataFrames will be converted to a string format (e.g., CSV or markdown table) for inclusion in the prompt.
   * Example: "Here is the most recent high-conviction options flow data for ticker XYZ: {filtered_polars_df_string}. Here is the current spot gamma exposure by strike for XYZ: {gex_by_strike_df_string}. The current stock price is {stock_price} and the overall market net GEX is {market_gex_value}."
4. Task & Instructions: This is a clear, step-by-step directive of what the model should do.
   * Example: "Analyze the provided data. First, provide a concise (2-3 sentences) 'Market Vibe' summary of the prevailing sentiment based on the flow and gamma positioning. Second, identify the top 1 to 3 potential trading opportunities. For each opportunity, provide your detailed reasoning, linking specific data points from the flow and GEX tables to your conclusion. Suggest a specific, defined-risk strategy that aligns with your analysis."
5. Output Format Specification: This is the most critical part for automation. The prompt must demand the output in a specific, machine-readable format, such as JSON. This eliminates the need for fragile string parsing and makes the integration with the rest of the application robust and reliable.
   * Example: "Your final output MUST be a single, valid JSON object. Do not include any text, explanations, or markdown formatting before or after the JSON block. The JSON object must adhere to the following schema: {\"analysis_summary\": \"<Your 2-3 sentence summary here>\", \"trade_ideas\":, \"risk_factors\": \"<string, e.g., 'Upcoming earnings, high implied volatility.'>\"}]}"
By enforcing a strict JSON output, the application can simply use a standard json.loads() function to parse the AI's response into a predictable Python dictionary. This transforms the LLM from a conversational chatbot into a reliable component of an automated workflow, a crucial distinction for building professional-grade software.


Section 3.3: Validating the Oracle: Mitigating LLM Risk in a Financial Context


An LLM is a probabilistic model, not a deterministic calculator. It is trained to generate plausible-sounding text, which is not the same as factually correct or logically sound analysis.9 In a financial context, where precision is paramount, blindly trusting an LLM's output is an unacceptable risk. LLMs can "hallucinate" facts, make mathematical errors, or misinterpret context.10 Therefore, a rigorous, automated validation layer must be implemented to scrutinize every response from Gemini before it is presented to the user. This layer acts as an automated supervisor, catching errors that a human analyst would spot instantly.
The validation pipeline will consist of a series of sequential checks. If a response fails any check, it is logged for review and discarded, never reaching the user's UI.
1. JSON Schema and Syntax Validation: The very first step is to try and parse the LLM's text response as JSON. If it fails (due to syntax errors, extra text, etc.), the response is invalid. If it parses successfully, the resulting object is then validated against the predefined JSON schema. Does it have the required keys (analysis_summary, trade_ideas)? Are the nested objects structured correctly? This ensures the output is in the expected format.
2. Numerical Sanity Checks: LLMs can struggle with precise numerical reasoning.11 This check verifies the numbers in the trade suggestion.
   * Are all strike prices, quantities, and prices positive numerical values?
   * For a bull call debit spread, is the strike of the 'BUY' leg lower than the strike of the 'SELL' leg?
   * For a bear put debit spread, is the strike of the 'BUY' leg higher than the strike of the 'SELL' leg?
   * Are the expiration dates valid and in the future?
3. Market Data Cross-Reference: This step validates the AI's suggestion against reality. It protects against hallucinations where the LLM might invent a ticker symbol or an option contract that doesn't exist.
   * The system will make a quick, secondary API call (to either Unusual Whales or IBKR) to fetch the actual option chain for the suggested ticker and expiration dates.
   * It will then verify: Does the suggested ticker ("XYZ") actually trade options? Do the suggested strike prices (150.0, 155.0) exist for the specified expiration date? If the option chain does not contain the suggested contracts, the idea is flagged as invalid.
4. Logical Consistency Check: This check ensures the AI's reasoning aligns with its recommendation.
   * Does the strategy_type match the sentiment expressed in the reasoning? If the reasoning describes "overwhelmingly bullish call buying and positive gamma," but the suggested strategy is a Bear Put Spread, the trade idea is logically inconsistent and should be discarded.
5. User Risk Parameter Check: The final check ensures the suggested trade conforms to the user's global risk settings.
   * Does the suggested spread width (difference between strikes) exceed a user-defined maximum?
   * Is the dte (days to expiration) within the user's preferred range?
   * Does the strategy type (Debit Spread, Calendar Spread) match the user's allowed strategies?
Only a trade idea that successfully passes every stage of this validation pipeline is considered a "confirmed trade idea" and is forwarded to the user's control panel for final review. This multi-stage validation process is a non-negotiable component of responsible AI integration in a financial application. It transforms the LLM from an untrusted "black box" into a powerful but supervised suggestion engine, mitigating a significant source of potential risk.


Part IV: The Execution Gateway: From Signal to Live Order


The final stage of the application's workflow is the execution of trades. This involves translating a validated, user-confirmed trade idea into a correctly formatted order and transmitting it to the market via the Interactive Brokers (IBKR) Trader Workstation (TWS) API. This layer must handle complex order construction, robust connection management, and real-time monitoring of order status and account positions.


Section 4.1: Mastering the Interactive Brokers TWS API


The IBKR TWS API is a powerful but complex interface. It is a TCP socket-based protocol, requiring careful management of the connection and message stream.43 A dedicated Execution Gateway service will encapsulate all this complexity.
Connection and Session Management:
The core of the Python API interaction revolves around a class that inherits from both EClient and EWrapper.44
* EClient is responsible for sending requests to TWS (e.g., place order, request market data).
* EWrapper is a callback interface that receives messages back from TWS (e.g., order status updates, market data ticks).
The gateway service will manage a persistent connection to a running instance of TWS or IB Gateway. It is essential that the brokerage application is configured correctly: "Enable ActiveX and Socket Clients" must be checked, and "Read-Only API" must be unchecked to permit trading.45 The service will handle the initial connection handshake, including processing the
nextValidId callback, which provides the starting order ID for the session.46 The entire process will run within an
asyncio-compatible thread to process incoming messages without blocking the main application.
Constructing Complex Spread Contracts:
The primary focus of this application is on defined-risk spreads like debit and calendar spreads. In the IBKR API, these multi-leg strategies are not placed as separate orders but as a single "combo" or "bag" order.47 This ensures that all legs are executed together as a single transaction at a specified net price.
Constructing a combo contract is a multi-step process:
1. Define the Combo Contract: Create a Contract object. For a spread, the secType must be set to "BAG".47 The
symbol can be set to the underlying ticker (e.g., 'AAPL'), and the exchange should be set to "SMART" for smart routing.
2. Fetch Leg conIds: The most crucial and often overlooked step is that each individual option leg of the spread needs its unique contract identifier, or conId. The conId for the AAPL 190 Call is different from the AAPL 195 Call. Before the BAG contract can be created, the application must first send reqContractDetails requests for each individual leg to get their respective conIds.
3. Populate ComboLegs: Once the conIds are retrieved, create a ComboLeg object for each leg of the spread. Each ComboLeg requires:
   * conId: The unique ID of the option contract for that leg.
   * ratio: The number of contracts for this leg (typically 1).
   * action: 'BUY' or 'SELL'.
   * exchange: 'SMART'.
   4. Attach Legs to Contract: Assign the list of ComboLeg objects to the comboLegs attribute of the main BAG Contract object.48
This process is complex and error-prone if done manually each time. Therefore, the Execution Gateway should contain helper functions to abstract this logic. The following table provides a guide to the required functions.


Strategy Name
	Description
	Python Function Signature
	Bull Call Debit Spread
	Buy a call with a lower strike, sell a call with a higher strike. Same expiry. A bullish, defined-risk strategy. 49
	async def create_bull_call_spread(self, symbol, expiry, lower_strike, upper_strike) -> Contract:
	Bear Put Debit Spread
	Buy a put with a higher strike, sell a put with a lower strike. Same expiry. A bearish, defined-risk strategy. 49
	async def create_bear_put_spread(self, symbol, expiry, lower_strike, upper_strike) -> Contract:
	Long Call Calendar Spread
	Sell a near-term call, buy a far-term call. Same strike. A neutral to bullish strategy profiting from time decay and/or rising volatility. 51
	async def create_long_call_calendar(self, symbol, strike, near_expiry, far_expiry) -> Contract:
	Each of these functions would internally handle the reqContractDetails calls to get the conIds and assemble the final BAG Contract object, turning a complex procedure into a simple, reusable function call.


Section 4.2: The Full Execution Workflow


With the foundational components in place, the end-to-end execution workflow can be defined. This workflow begins the moment the user confirms a trade and ends with the position being actively monitored in the portfolio.
From User Confirmation to Live Order:
Once the user reviews a validated trade idea from Gemini and clicks the "Confirm Trade" button in the UI, the following sequence is initiated within the Execution Gateway:
   1. Contract Creation: The system calls the appropriate helper function from the Spread Construction Guide (e.g., create_bull_call_spread) with the parameters from the trade idea. This function asynchronously retrieves the necessary conIds and returns a fully formed BAG Contract object.
   2. Order Creation: An Order object is created.
   * action: For a debit spread, the action is 'BUY'.
   * orderType: This will typically be 'LMT' (Limit Order) to control the execution price. For a debit spread, the trader is buying the spread, so the limit price represents the maximum net debit they are willing to pay.53
   * totalQuantity: The number of spreads to trade.
   * lmtPrice: The net price for the spread. This can be derived from the mid-point of the spread's current bid-ask or specified by the user.
   * transmit: This must be set to True to send the order to the exchange.47
   3. Order Placement: The system calls self.placeOrder(self.nextOrderId, contract, order), using the next available order ID. The nextOrderId is then incremented for the next trade.46
Order and Position Monitoring:
Placing an order is not the end of the process. A professional system must continuously monitor its state. The Execution Gateway will implement the necessary EWrapper callback methods to handle messages from TWS:
   * orderStatus: This callback is invoked by TWS whenever the status of an order changes (e.g., Submitted, Filled, Cancelled). The application will use this to update the UI in real-time, showing the user the current state of their working orders.46 It's important to handle various statuses and filter out potential duplicate messages.
   * execDetails: When an order is partially or fully filled, the execDetails callback provides the specifics of the execution, including the execution ID, fill price, and quantity.46 This is the definitive confirmation that a trade has occurred.
   * openOrder: This provides a complete snapshot of all open orders upon request or when an order is first placed.46
Portfolio and Account Updates:
To provide a complete view of the trading account and to power the in-flight risk management controls, the system must be aware of current positions and account values. Upon startup, the Execution Gateway will call reqAccountUpdates(True, account_id). This subscribes the client to a stream of updates. TWS will then continuously push messages to the updatePortfolio and updateAccountValue callbacks.44
   * updatePortfolio: Provides real-time information on every position held in the account, including the contract, position size, market price, and unrealized P&L.
   * updateAccountValue: Provides key account metrics like NetLiquidation, TotalCashValue, and MaintMarginReq. The NetLiquidation value is essential for the position sizing and max drawdown risk calculations.
By implementing this full workflow, the Execution Gateway becomes a robust and reliable interface to the market, capable of not only executing complex spread orders but also providing the real-time feedback necessary for effective position monitoring and risk management.


Part V: Conclusion and Path Forward


This report has laid out a comprehensive architectural blueprint for a sophisticated, AI-driven options trading application. By moving beyond the concept of a simple script to embrace a professional, event-driven architecture, the resulting system is designed for robustness, scalability, and performance. The integration of specialized tools—Unusual Whales for high-quality data, Polars for high-performance processing, Google Gemini for nuanced analysis, and Interactive Brokers for reliable execution—creates a powerful synergy.
The design philosophy is rooted in several key principles that distinguish a professional system from an amateur one. The adoption of an Event-Driven Architecture decouples the system's components, ensuring that a delay or failure in one part does not cascade and bring down the entire application. The use of asynchronous operations is not merely an optimization but a fundamental requirement to handle the inherent latencies of network-bound APIs, particularly the call to the Gemini model, ensuring the system remains responsive to real-time market events.
In the data layer, the strategy moves beyond simply consuming data to intelligently selecting it. The focus on the /option-trades/flow-alerts endpoint and, critically, the use of real-time, directionalized gamma exposure from /stock/:ticker/spot-exposures/strike provides a significant analytical edge over static, daily data. In the intelligence layer, the emphasis on structured JSON output via meticulous prompt engineering transforms the LLM from a conversational tool into a reliable, programmatic component. This is fortified by the AI validation layer, a non-negotiable supervisory process that mitigates the inherent risks of LLM "hallucinations" and numerical inaccuracies by subjecting every AI-generated idea to a rigorous battery of sanity and market-data checks. Finally, the entire structure is built upon a multi-layered risk management framework, embedding controls at the pre-trade, in-flight, and model-level to protect capital, which is the ultimate priority of any trading endeavor.
Path to Production:
Building the system described is a significant undertaking. A disciplined, phased approach to deployment is critical to ensure its stability and reliability before committing real capital.
   1. Backtesting: The core signal generation logic—the Polars pipeline that identifies high-conviction flow and the Gemini prompts that generate trade ideas—must be rigorously tested against historical data. The Unusual Whales API provides access to historical flow data, which can be used to simulate the data ingestion process.12 By running the analytical core against months or years of past data, one can evaluate the historical performance of the strategy, refine the filtering criteria, and tune the AI prompts. This phase is crucial for building confidence in the strategy's efficacy.
   2. Paper Trading: Once the strategy shows promise in backtesting, the complete, end-to-end system must be deployed and connected to an Interactive Brokers paper trading account.45 This phase tests the system in a live market environment without financial risk. It is essential for identifying and resolving bugs in the execution logic, connection management, and real-time data handling. This phase should be run for an extended period (weeks or months) to ensure the system behaves as expected across various market conditions.
   3. Incremental Live Deployment: After successful and stable performance in a paper trading environment, the system can be deployed with real capital. It is prudent to begin with a small, controlled allocation of capital. The position sizing should be set to its most conservative level. As the system demonstrates consistent and reliable performance over time in the live market, the position size and capital allocation can be gradually and cautiously increased.
By adhering to this professional-grade blueprint and following a disciplined path to production, it is possible to construct a trading application that is not only powerful and intelligent but is fundamentally robust, scalable, and designed with risk management as its central pillar. This approach provides the foundation for transforming a sophisticated trading concept into a tangible and potentially profitable reality.
Works cited
   1. Algorithmic Trading System Architecture - Stuart Gordon Reid - Turing Finance, accessed July 31, 2025, http://www.turingfinance.com/algorithmic-trading-system-architecture-post/
   2. bradleyboyuyang/Trading-System: An asynchronous low-latency trading system - GitHub, accessed July 31, 2025, https://github.com/bradleyboyuyang/Trading-System
   3. How to Build a Low-Latency Trading Infrastructure (in 6 Steps) - ForexVPS, accessed July 31, 2025, https://www.forexvps.net/resources/low-latency-trading-infrastructure/
   4. Low Latency Trading Architecture - QCon London, accessed July 31, 2025, https://qconlondon.com/london-2017/system/files/presentation-slides/lmax-architecture.pdf
   5. Polars — DataFrames for the new era, accessed July 31, 2025, https://pola.rs/
   6. Importance of Risk Management in Algo Trading - uTrade Algos, accessed July 31, 2025, https://www.utradealgos.com/blog/risk-management-in-algo-trading
   7. 7 Risk Management Strategies For Algorithmic Trading | Nurp, accessed July 31, 2025, https://nurp.com/wisdom/7-risk-management-strategies-for-algorithmic-trading/
   8. Risk Mitigation Techniques and Best Practices With Automated Trading - PineConnector, accessed July 31, 2025, https://www.pineconnector.com/blogs/pico-blog/risk-mitigation-techniques-and-best-practices-with-automated-trading
   9. Steps to Effectively Validate Your LLM Application - Deepchecks, accessed July 31, 2025, https://www.deepchecks.com/steps-to-effectively-validate-your-llm-application/
   10. The complete guide to LLM model evaluation: From baseline testing to real-world deployment - NayaOne, accessed July 31, 2025, https://nayaone.com/knowledgebase/llm-model-evaluation-in-financial-services-full-guide/
   11. Can Large language model analyze financial statements well? - ACL Anthology, accessed July 31, 2025, https://aclanthology.org/2025.finnlp-1.19.pdf
   12. Public API - Get access to options flow and stock data via API - Unusual Whales, accessed July 31, 2025, https://unusualwhales.com/public-api
   13. UW - API - Unusual Whales, accessed July 31, 2025, https://api.unusualwhales.com/docs
   14. Unusual Whales API Example - flow alerts multiple tickers v3, accessed July 31, 2025, https://unusualwhales.com/public-api/examples/flow_alerts_multiple_tickers
   15. Use the flow alerts endpoint from the Unusual Whales API to get urgent screen trades that were likely openers - gist/GitHub, accessed July 31, 2025, https://gist.github.com/danwagnerco/84aca96cca09820033fe8006e1434082
   16. How To Read the Directionalized Volume Spot Gamma Exposure Chart - Unusual Whales, accessed July 31, 2025, https://unusualwhales.com/information/how-to-read-the-directionalized-volume-spot-gamma-exposure-chart
   17. Unusual Whales API Example - custom alerts demo, accessed July 31, 2025, https://unusualwhales.com/public-api/examples/custom-alerts
   18. How To Check Your API Usage - Unusual Whales, accessed July 31, 2025, https://unusualwhales.com/information/how-to-check-your-api-usage
   19. 8 ways pandas really losing to Polars for quick market data analysis - PyQuant News, accessed July 31, 2025, https://www.pyquantnews.com/the-pyquant-newsletter/8-ways-pandas-losing-polars-quick-data-analysis
   20. G-Research tackles the biggest financial data challenges of tomorrow with Polars, accessed July 31, 2025, https://pola.rs/posts/case-gresearch/
   21. Polars DataFrame.cast() Method with Examples, accessed July 31, 2025, https://sparkbyexamples.com/polars/polars-dataframe-cast-method-with-examples/
   22. polars.DataFrame.cast — Polars documentation, accessed July 31, 2025, https://docs.pola.rs/py-polars/html/reference/dataframe/api/polars.DataFrame.cast.html
   23. 3 Performance – Modern Polars - GitHub Pages, accessed July 31, 2025, https://kevinheavey.github.io/modern-polars/performance.html
   24. An Introduction to Polars: Python's Tool for Large-Scale Data Analysis - GeeksforGeeks, accessed July 31, 2025, https://www.geeksforgeeks.org/python/an-introduction-to-polars-pythons-tool-for-large-scale-data-analysis/
   25. Getting started - Polars user guide, accessed July 31, 2025, https://docs.pola.rs/user-guide/getting-started/
   26. Spotting Whale Activity: How Options Volume Spikes Reveal Big Moves, accessed July 31, 2025, https://www.optionstrading.org/blog/how-to-identify-whale-activity-options/
   27. How to Find Unusual Stocks Options Activity - Bookmap, accessed July 31, 2025, https://bookmap.com/blog/spotting-unusual-options-activity-predicting-stock-movements-for-trading-success
   28. WhaleStream Real-time Options Trading Activity, accessed July 31, 2025, https://www.whalestream.com/
   29. Frequently Asked Questions - Unusual Whales, accessed July 31, 2025, https://docs.unusualwhales.com/faq/
   30. Introduction to Polars - Practical Business Python -, accessed July 31, 2025, https://pbpython.com/polars-intro.html
   31. Quick Intro to Polars- For Feature Engineering | by Vidyasagar Bhargava | Medium, accessed July 31, 2025, https://medium.com/@vidyasagarbhargava/quick-intro-to-polars-for-feature-engineering-054b6d6e868e
   32. What Is Unusual Options Activity And What Does It Tell You? - Bankrate, accessed July 31, 2025, https://www.bankrate.com/investing/unusual-options-activity/
   33. How to Spot Buying Opportunities in Options Order Flow - Nasdaq, accessed July 31, 2025, https://www.nasdaq.com/articles/how-to-spot-buying-opportunities-in-options-order-flow
   34. Options Flow Feed Page Breakdown - Unusual Whales, accessed July 31, 2025, https://unusualwhales.com/information/options-flow-feed-page-breakdown
   35. Options Order Flow: Everything You Need To Know, accessed July 31, 2025, https://www.cheddarflow.com/blog/options-order-flow-everything-you-need-to-know/
   36. support.spotgamma.com, accessed July 31, 2025, https://support.spotgamma.com/hc/en-us/articles/33608294279955-What-is-GEX#:~:text=Gamma%20Exposure%20(abbreviated%20as%20GEX,gamma%20held%20by%20market%20makers.
   37. Options Trading Secrets: Supercharge Gains with Gamma Exposure - YouTube, accessed July 31, 2025, https://www.youtube.com/watch?v=YATD7GdUAXs
   38. S&P 500 Index Gamma Exposure (GEX) - Barchart.com, accessed July 31, 2025, https://www.barchart.com/stocks/quotes/%24SPX/gamma-exposure
   39. GEX — 指標和策略 - TradingView, accessed July 31, 2025, https://tw.tradingview.com/scripts/gex/?script_access=all
   40. GEX Levels 1 to 10 - Menthor Q, accessed July 31, 2025, https://menthorq.com/guide/gex-levels/
   41. Prompt Engineering for AI Guide | Google Cloud, accessed July 31, 2025, https://cloud.google.com/discover/what-is-prompt-engineering
   42. 5 tips to master the art of prompt engineering with Gemini for Workspace - SADA, accessed July 31, 2025, https://sada.com/blog/5-tips-to-master-the-art-of-prompt-engineering-with-gemini-for-workspace/
   43. TWS API Documentation | IBKR API | IBKR Campus, accessed July 31, 2025, https://www.interactivebrokers.com/campus/ibkr-api-page/trader-workstation-api/
   44. Python TWS API | Trading Course | Traders' Academy - Interactive Brokers LLC, accessed July 31, 2025, https://www.interactivebrokers.com/campus/trading-course/python-tws-api/
   45. Installing & Configuring TWS for the API - Interactive Brokers LLC, accessed July 31, 2025, https://www.interactivebrokers.com/campus/trading-lessons/installing-configuring-tws-for-the-api/
   46. Placing Orders using TWS Python API | Trading Lesson - Interactive Brokers LLC, accessed July 31, 2025, https://www.interactivebrokers.com/campus/trading-lessons/python-placing-orders/
   47. TWS Python API Placing Complex Orders | Trading Lesson - Interactive Brokers LLC, accessed July 31, 2025, https://www.interactivebrokers.com/campus/trading-lessons/python-complex-orders/
   48. Placing a Bull Put Spread order with IBKR's TWS API using ibapi - Stack Overflow, accessed July 31, 2025, https://stackoverflow.com/questions/79490048/placing-a-bull-put-spread-order-with-ibkrs-tws-api-using-ibapi
   49. Lesson 13: What are debit spreads? - CIBC Investor's Edge, accessed July 31, 2025, https://www.investorsedge.cibc.com/en/learn/options-trading-course/debit-spreads.html
   50. Debit Spread Options Strategies | TrendSpider Learning Center, accessed July 31, 2025, https://trendspider.com/learning-center/debit-spread-options-strategies/
   51. www.ig.com, accessed July 31, 2025, https://www.ig.com/uk/listed-options-futures/options-need-to-knows/calendar-spread#:~:text=A%20calendar%20spread%20is%20an,both%20be%20calls%20or%20puts.
   52. Understanding the calendar spread strategy - Saxo Bank, accessed July 31, 2025, https://www.home.saxo/learn/guides/options/understanding-the-calendar-spread-strategy
   53. Debit Spread: Definition, Example, Vs. Credit Spread - Investopedia, accessed July 31, 2025, https://www.investopedia.com/terms/d/debitspread.asp
   54. Option Combinations | Trading Lesson | Traders' Academy - Interactive Brokers LLC, accessed July 31, 2025, https://www.interactivebrokers.com/campus/trading-lessons/option-combinations/